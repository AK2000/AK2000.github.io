<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://ak2000.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ak2000.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-18T02:08:30+00:00</updated><id>https://ak2000.github.io/feed.xml</id><title type="html">blank</title><subtitle>A limited internet home to prove I exist. As professional and academic as I can be, maybe, I hope. Based on [*folio](https://github.com/bogoli/-folio) design.</subtitle><entry><title type="html">What I’ve Been Reading (1/17/2025)</title><link href="https://ak2000.github.io/blog/2025/readings/" rel="alternate" type="text/html" title="What I’ve Been Reading (1/17/2025)"/><published>2025-01-17T17:30:00+00:00</published><updated>2025-01-17T17:30:00+00:00</updated><id>https://ak2000.github.io/blog/2025/readings</id><content type="html" xml:base="https://ak2000.github.io/blog/2025/readings/"><![CDATA[<h3 id="architectural-implications-of-function-as-a-service-computing">Architectural Implications of Function-as-a-Service Computing</h3> <table> <tbody> <tr> <td>Mohammad Shahrad, Jonathan Balkind, David Wentzlaff</td> <td>Micro 2019</td> <td>1/7/2025</td> </tr> </tbody> </table> <hr/> <p><strong><em>Summary:</em></strong> Serverless computing provides a popular paradigm where developers only pay for resources they use, and providers manage provisioning and autoscaling. This paradigm creates a new kind of workload for the cloud with (1) very short functions, (2) fine-grained billing based on GB-s, (3) fine-grained inter leaving. This workload challenges some of the assumptions of OS and server architecture. They find:</p> <ol> <li>Cold start costs are non-negligible and require increased memory-bandwidth, a well known phenomenon for which much research has been done since</li> <li>Branch predictors perform badly for very short functions, causing high branch misprediction</li> <li>Function execution time can be on the order of OS scheduling quantum</li> <li>Without SLAs on latency, the provider has incentive to over sell capcity, creating slow-downs for the user, and making the provider more money.</li> <li>Function interfence can cause increase in page faults, context switches and LLC misses.</li> </ol> <p><strong><em>Thoughts:</em></strong> Many of the things recongized in this paper see obvious, sepcifically cold starts, function interference, and the interplacy between capacity and cost. The increase in branch prediction misses is notable, more so because I thought that these kind of issues would be the same issues faced by say oversubscribing cores for multi-processing. The issue of OS scheduling for short functions is also conerning and relevant. This again plays into costs and incentives. Users are charged for the runtime of their function, which is dependent on the CPU slots allocated to them. If the OS scheduling overhead of Linux CFS is on the same order of magnitude as the function execution, users could be paying significantly more because of the scheduler configuration. This problem is addresed in a new paper: <em>In Serverless, OS Scheduler Choice Costs Money: A Hybrid Scheduling Approach for Cheaper FaaS</em> which may be a paper for a different time.</p> <hr/> <hr/> <h3 id="will-serverless-end-the-dominance-of-linux-in-the-cloud">Will Serverless End The Dominance of Linux in the Cloud</h3> <table> <tbody> <tr> <td>Richard Koller, Dan Williams</td> <td>HotOS 17</td> <td>1/9/2025</td> </tr> </tbody> </table> <hr/> <p><strong><em>Summary:</em></strong> Again on the serverless theme. The authors make a distinction in the requirements for “actions” which are the unit of serverless computing, and processes which are the unit the linux kernel was designed for. They lay out latency and throughput requirements for serverless based on AWS lambda cost: an action must be able to complete in 100ms, and a 4 core server must be able to support 125 actions per second. They compare different solutions fer serverless architectures: a native linux process, a container, and a unikernel. The authors show that a container takes almost 6 times longer to boot than a process, but a unikernel takes only 34\% more time. Furthermore, they find the max throughput for a container to be 24 tasks/sec, compared to 329 for a process. As possible solutions they propose we could (1) bypass the kernel, as is done in a unikernel, or (2) replace the kernel with something that provides less features, no preemptable scheduling, no IPC, and no process sychronization features.</p> <p><strong><em>Thoughts:</em></strong> Its interesting to see how serverless can shape and change the direction of the OS, but my thought is that the changes proposed here are too radical. The authors undervalue the history and extensive community that linux comes with. These things provide confidence that the code in the kernel is well-tested, secure, and will be maintained over a long term. I also think that we have seen other, less radical solutions to the cold-start problem in serverless. The authors dismiss caching as a inelegant solution, but this has been used with great effectiveness. Furthermore, I am interested to know what portion of these problems are addressed with a MicroVM like firecracker.</p> <hr/> <hr/> <h3 id="unikernels-library-operating-systems-for-the-clouds">Unikernels: Library Operating Systems for the Clouds</h3> <p>Anil Madhavapeddy, Richard Mortier, Charalampos Rotsos, David Scott, Balraj Singh, Thomas Gazagnaire, Steven Smith, Steven Hand and Jon Crowcroft | ASPLOS 2013 | 1/13/25</p> <hr/> <p><strong><em>Summary:</em></strong> Cloud appliances often sepearte different components of an application into sepearte VMs (a database, a webserver, etc.). The authors propose unikernels, single application virtual machine where operating system functionality is linked directly to the application. This ahs advantages for size, and for safety since the application can be statically type checked, privleges can be dropped from the VM to prevent executable pages and the VM does not need process level permission management. The paper containes details of the simplifications that a unikernel allows to the memory management and garbage collector of the OCaml runtime as well as details for how drivers and block devices work for the unikernel.</p> <p><strong><em>Thoughts:</em></strong> The idea here is very simple, but required a lot of engineering effort to bring to fruition. Since this paper, unikernels have become an influential idea in systems, although they still remain difficult to use in practice. I still don’t understand how certain parts of the unikernel/OS work. What does it mean to implement the scheduler as a library function? Why exactly were the unikernels in this paper restricted to a single heap?</p> <hr/> <hr/> <h3 id="with-great-freeedom-comes-great-opportunity-rethinking-resource-allocation-for-serverless-functions">With Great Freeedom Comes Great Opportunity: Rethinking Resource Allocation for Serverless Functions</h3> <p>Muhammad Bilal, Marco Canini, Rodrigo Fonseca, Rodrigo Rodrigues | EuroSys 23 | 1/15/25</p> <hr/> <p><strong><em>Summary:</em></strong> In this paper, the authors examine the resource allocation model provided by current serverless platforms, which couples CPU resources to amount of allocated memory, and typically does not provide the oportunity to configure CPU types. The authors project the cost model from current offerings onto heterogeneous offerings, both in terms of instance type, and in terms of CPU vs. memory tradeoff. They show that by decoupling memory and providing different CPU options, functions can be executed cheaper and faster. They then move to the feasibility of predicting the “right” resource allocation for a serverless function using saympling, and in an online setting using bayseian optimization. They also find that the input data only has a modest affect on the best configuration for a function. Finally they consider the resrouce allocation problem from the providers perspective. They provide three different options on how to make different configurations available to the user: give users options on the predicted pareto front, provide an optimization with cost and execution time weighted differently, and provide a hierarchical optimization that minimizes cost under a constaint of execuion time.</p> <p><strong><em>Thoughts:</em></strong> This is an interesting and well-done paper, but I believe it misses one of the key reasons that serverless is feasible from a providers perspective. It’s unsuprising that there are better configurations that those provided by the default memory/CPU proportional design; I don’t think anyone expected a static proportion of CPU to memory would be optimal for all functions. However, such a static tradeoff drastically simplifies the placement of serverless functions. Rather than having a multi-attribute bin-packing problem when shcuedling functions, the problem is reduced to fitting functions into static “slots”. This can easily ensure that all space on a server is sold, both in terms of memory and CPU. From a providers perspective, there is not really an incentive to optimize this space more, because it complicates scheduling and could lead to stranded resources – a persistent issue when allocating traditional cloud instances. The most interesting part of the paper to revisit is the bayesian optimization, and the online optimization sections, which could be relevant for task-based runtimes, or FuncX, a serverless system that doesn’t deal with “selling” time.</p>]]></content><author><name></name></author><category term="professional"/><category term="readings"/><summary type="html"><![CDATA[A bi-weekly recap of the papers that I've been reading]]></summary></entry><entry><title type="html">Hello World!</title><link href="https://ak2000.github.io/blog/2024/new-years/" rel="alternate" type="text/html" title="Hello World!"/><published>2024-12-31T17:30:00+00:00</published><updated>2024-12-31T17:30:00+00:00</updated><id>https://ak2000.github.io/blog/2024/new-years</id><content type="html" xml:base="https://ak2000.github.io/blog/2024/new-years/"><![CDATA[<p>Hello! Welcome to my blog. It’s very unlike me to write a blog, or to have any online presence at all, but I’m trying something new for the new year. I’m not sure yet what this will look like, but I’m thinking some mix of professional and personal thoughts. Essentially, this will be a space for me to make personal notes and a some what of a research journal. Over the last year, I’ve been frustrated with some of my research progress — not necessarily how things are going or what I’m doing but definitely the results, in terms of writing and publications. In the new year, I want to focus more on the process, which is something that I feel I can control, and something I feel that I can manage better. I envisage this blog as part of that.</p> <h3 id="goals-for-this-blog">Goals for this blog:</h3> <ul> <li> <p><em>Practice writing</em>: I repeatedly have heard the advice that Ph.D. students need more practice writing than they wil get through just papers and theses. I’m going to try to add an entry to this blog every other week: Giving myself some room for slipping, That’s 25 entries this year. I haven’t tried this before, so I will have to revaluate how I’m doing later in the yaer.</p> </li> <li> <p><em>Document reading</em>: Taking inspiration from Adrian Coyer’s “The Morning Paper”, this year I am going to try to read one paper every other day. I’m not going to be as ambitious in thinking that I will be able to do an in depth write up for everything that I read. But I aim to do one post a month that is a brief documentation of all of the papers that I read that month, as well as an in depth write up about one of the papers.</p> </li> <li> <p><em>Document reasearch</em>: One of the other things that I think happened over the past year is that I have spent time working on things that haven’t turned into a project or anything publishable or presentable. This has included things like: learning how to build linux kernel modules, figuring out what made Ray applications more energy efficient (it was a detail with the interaction between OpenMP flags and python multiprocessing), and experiments with skiping layers in transformer models. I don’t think anyone should listen to my wisdom, but I believe that these tasks are part of the research process. If we knew what all of the interesting problems were, or which ideas were going to bear fruit before we started, than there would not be much interesting research left to do. But most of these side journeys do not turn into anything real. Thus, I’m going to try document the things that I’m exploring in this blog. Most of these things will be uninteresting to most people. but I’m doing this mainly for my own sanity, so I can reflect on how I’ve been spending my time, and hopefully revisit these ideas at some point in the future.</p> </li> </ul> <p>Anyway, that’s the idea behind this blog, and what finally motivated me to set up a personal website. I’m also going to be trying to flesh out some of the other parts of my site this year, which may take the place of a few of these blog entries, we’ll see.</p>]]></content><author><name></name></author><category term="personal"/><category term="meta"/><summary type="html"><![CDATA[A first blog post and some new years thoughts]]></summary></entry></feed>